{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Segmentation with SageMaker\n",
    "\n",
    "This notebook walks through the process of building a user segmentation model using AWS SageMaker. We will use the K-Means algorithm to cluster users based on their data from Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Initialization\n",
    "\n",
    "Import necessary libraries, initialize SageMaker session, and define S3 buckets and roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "# TODO: Replace with your S3 bucket names\n",
    "data_bucket = 'my-awesome-project-csv-bucket-dev' \n",
    "model_artifacts_bucket = 'my-awesome-project-sagemaker-artifacts-dev'\n",
    "\n",
    "print(f'SageMaker role is: {role}')\n",
    "print(f'Using data bucket: {data_bucket}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Extraction and Preprocessing\n",
    "\n",
    "Load data from Redshift (user data and CRM data). Then, preprocess the data to prepare it for training. This might involve handling missing values, feature scaling, and one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code to connect to Redshift and load data\n",
    "# You can use libraries like psycopg2 or SQLAlchemy\n",
    "\n",
    "# Example: Load data into a pandas DataFrame\n",
    "# crm_data = pd.read_sql('SELECT * FROM crm_data', redshift_connection)\n",
    "# user_data = pd.read_sql('SELECT * FROM user_data', redshift_connection)\n",
    "\n",
    "# TODO: Merge and preprocess the data\n",
    "# combined_data = pd.merge(user_data, crm_data, on='user_id')\n",
    "# preprocessed_data = ...\n",
    "\n",
    "# TODO: Upload the preprocessed data to S3 for training\n",
    "# preprocessed_data.to_csv('train.csv', header=False, index=False)\n",
    "# sagemaker_session.upload_data('train.csv', bucket=data_bucket, key_prefix='user-segmentation/training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Use the SageMaker K-Means estimator to train the clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# Get the pre-built K-Means image\n",
    "container = get_image_uri(boto3.Session().region_name, 'kmeans')\n",
    "\n",
    "# Create the Estimator\n",
    "kmeans = Estimator(container,\n",
    "                   role,\n",
    "                   train_instance_count=1,\n",
    "                   train_instance_type='ml.c4.xlarge',\n",
    "                   output_path=f's3://{model_artifacts_bucket}/user-segmentation/output',\n",
    "                   sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Set hyperparameters\n",
    "kmeans.set_hyperparameters(k=5, # Example: 5 clusters\n",
    "                           feature_dim=10) # TODO: Set the correct feature dimension\n",
    "\n",
    "# Train the model\n",
    "# train_data_path = f's3://{data_bucket}/user-segmentation/training'\n",
    "# kmeans.fit({'train': train_data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Deployment\n",
    "\n",
    "Deploy the trained model to a SageMaker endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "# kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n",
    "#                                instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference and Cleanup\n",
    "\n",
    "Make predictions with the deployed endpoint and then delete the endpoint to avoid incurring costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "# test_data = [...] \n",
    "# result = kmeans_predictor.predict(test_data)\n",
    "# print(result)\n",
    "\n",
    "# Clean up the endpoint\n",
    "# sagemaker_session.delete_endpoint(kmeans_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}